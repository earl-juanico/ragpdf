{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad3b0d1",
   "metadata": {},
   "source": [
    "### Test_Rag_PH\n",
    "\n",
    "The task involves the following:\n",
    "* Extract the textual and graphical information from the PDF pages.\n",
    "* Convert the extracted graphical data (such as charts or graphs) into a structured, queryable format.\n",
    "* Implement a system where users can ask questions and receive meaningful responses based on the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecbc96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytesseract\n",
    "from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration\n",
    "\n",
    "try:\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    _have_dbscan = True\n",
    "except Exception:\n",
    "    _have_dbscan = False\n",
    "\n",
    "MONTH_NAMES = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "month_name_to_idx = {m.lower(): i for i, m in enumerate(MONTH_NAMES)}\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def load_models(device):\n",
    "    deplot_proc = Pix2StructProcessor.from_pretrained(\"google/deplot\")\n",
    "    deplot_model = Pix2StructForConditionalGeneration.from_pretrained(\"google/deplot\").to(device)\n",
    "    return deplot_proc, deplot_model\n",
    "\n",
    "def _project_root():\n",
    "    \"\"\"\n",
    "    Resolve a sensible project root in a notebook:\n",
    "      - prefer __file__ when available (script run)\n",
    "      - fallback to current working directory when running inside a notebook\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return Path(__file__).resolve().parent\n",
    "    except Exception:\n",
    "        return Path.cwd().resolve()\n",
    "\n",
    "def _ensure_markdown_generated(md_path):\n",
    "    \"\"\"\n",
    "    If requested markdown doesn't exist, attempt to run MinerU demo.parse_doc to produce it.\n",
    "    Looks for PDF(s) under MinerU/demo/pdfs and calls parse_doc -> output dir matching the demo layout.\n",
    "    \"\"\"\n",
    "    md_path = Path(md_path)\n",
    "    if md_path.exists():\n",
    "        return True\n",
    "\n",
    "    project_root = _project_root()\n",
    "    demo_py = project_root / \"MinerU\" / \"demo\" / \"demo.py\"\n",
    "    demo_output_dir = project_root / \"MinerU\" / \"demo\" / \"output\"\n",
    "    demo_pdfs_dir = project_root / \"MinerU\" / \"demo\" / \"pdfs\"\n",
    "\n",
    "    if not demo_py.exists():\n",
    "        print(\"demo.py not found at\", demo_py, \"- cannot auto-generate markdown.\")\n",
    "        return False\n",
    "\n",
    "    pdfs = list(demo_pdfs_dir.glob(\"*.pdf\")) if demo_pdfs_dir.exists() else []\n",
    "    if not pdfs:\n",
    "        print(\"No PDFs found under\", demo_pdfs_dir, \"— cannot run demo.parse_doc.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        spec = importlib.util.spec_from_file_location(\"mineru_demo\", str(demo_py))\n",
    "        demo_mod = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(demo_mod)\n",
    "        demo_mod.parse_doc([p for p in pdfs], output_dir=str(demo_output_dir), backend=\"pipeline\")\n",
    "        print(\"Ran demo.parse_doc; waiting for markdown generation.\")\n",
    "        return md_path.exists()\n",
    "    except Exception as e:\n",
    "        print(\"Failed to run demo.parse_doc:\", e)\n",
    "        return False\n",
    "\n",
    "def _resolve_md_path_input(md_input):\n",
    "    \"\"\"\n",
    "    Resolve a user-provided md_input:\n",
    "      - if it's an existing path, return it\n",
    "      - if it's just a filename (with or without .md), resolve to:\n",
    "          MinerU/demo/output/<stem>/auto/<stem>.md\n",
    "    \"\"\"\n",
    "    inp = str(md_input)\n",
    "    p = Path(inp).expanduser()\n",
    "    # if looks like a path or exists, use it\n",
    "    if p.exists() or (\"/\" in inp) or (\"\\\\\" in inp) or inp.startswith(\".\"):\n",
    "        return p.resolve()\n",
    "    # otherwise treat as filename and build demo output path\n",
    "    stem = Path(inp).stem\n",
    "    project_root = _project_root()\n",
    "    demo_output_dir = project_root / \"MinerU\" / \"demo\" / \"output\"\n",
    "    candidate = demo_output_dir / stem / \"auto\" / f\"{stem}.md\"\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b6611",
   "metadata": {},
   "source": [
    "The chart in the PDF only labels the starting and ending values along the horizontal axis, although ticks are provided in between. \n",
    "\n",
    "Converting this chart into structured data requires that each tick has a corresponding label. \n",
    "\n",
    "Below is a classic CV pipeline to detect the ticks from the chart image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "557473f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_ticks_classic(img_bgr, *,\n",
    "                         axis_search_frac=(0.55, 0.95),\n",
    "                         axis_min_length_frac=0.35,\n",
    "                         band_half_h_frac=0.02,\n",
    "                         tick_min_gap_frac=0.03,\n",
    "                         ocr_label_h_frac=0.12,\n",
    "                         edge_blur=3):\n",
    "    \"\"\"\n",
    "    Classic CV pipeline to find X-axis tick x positions and labels without foundation models.\n",
    "\n",
    "    Returns:\n",
    "      list of {\"x\": int, \"label\": str|None, \"bbox\": (x0,y0,x1,y1)}\n",
    "    Notes:\n",
    "      - All *_frac params are relative to image width/height (robust across sizes).\n",
    "      - Uses Hough (fast) with a projection fallback, then vertical projection + simple peak clustering.\n",
    "    \"\"\"\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    if edge_blur and edge_blur > 1:\n",
    "        gray = cv2.GaussianBlur(gray, (edge_blur|1, edge_blur|1), 0)\n",
    "\n",
    "    # 1) search lower area for a dominant horizontal axis\n",
    "    y0 = int(h * axis_search_frac[0])\n",
    "    y1 = int(h * axis_search_frac[1])\n",
    "    roi = gray[y0:y1, :]\n",
    "    edges = cv2.Canny(roi, 50, 150)\n",
    "\n",
    "    axis_y = None\n",
    "    min_len = int(w * axis_min_length_frac)\n",
    "    lines = cv2.HoughLinesP(edges, rho=1, theta=np.pi/180, threshold=60,\n",
    "                            minLineLength=min_len, maxLineGap=int(w*0.02))\n",
    "    if lines is not None and len(lines):\n",
    "        ys = []\n",
    "        for x1_, y1_, x2_, y2_ in lines.reshape(-1,4):\n",
    "            ys.append(int((y1_ + y2_)/2))\n",
    "        axis_y = int(np.median(ys)) + y0\n",
    "    else:\n",
    "        # fallback: horizontal projection (sum of edges) in ROI\n",
    "        proj = edges.sum(axis=1)\n",
    "        if proj.max() > 0:\n",
    "            rel = int(np.argmax(proj))\n",
    "            axis_y = rel + y0\n",
    "\n",
    "    if axis_y is None:\n",
    "        return []\n",
    "\n",
    "    # 2) Build narrow band around axis and compute vertical projection to find tick candidates\n",
    "    band_h = max(2, int(h * band_half_h_frac))\n",
    "    band_top = max(0, axis_y - band_h)\n",
    "    band_bot = min(h, axis_y + band_h)\n",
    "    band = gray[band_top:band_bot, :]\n",
    "\n",
    "    # use Sobel/edges to emphasize vertical ticks\n",
    "    sob = cv2.Sobel(band, cv2.CV_16S, 1, 0, ksize=3)\n",
    "    sob = cv2.convertScaleAbs(sob)\n",
    "    proj_v = sob.sum(axis=0).astype(np.float32)\n",
    "    # normalize and threshold relative to mean\n",
    "    thr = max(1.0, proj_v.mean() * 1.2)\n",
    "    peaks = np.where(proj_v > thr)[0]\n",
    "\n",
    "    if peaks.size == 0:\n",
    "        # fallback: connected components on thresholded band\n",
    "        _, bw = cv2.threshold(band, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        cc = cv2.connectedComponentsWithStats(bw, 8, cv2.CV_32S)\n",
    "        stats = cc[2]\n",
    "        centers = []\n",
    "        for s in stats[1:]:\n",
    "            x, y, ww, hh, area = s\n",
    "            # keep narrow tall components relative to band height\n",
    "            if hh >= max(3, band.shape[0]*0.4) and ww <= max(3, w*0.02):\n",
    "                centers.append(int(x + ww/2))\n",
    "        peaks = np.array(sorted(set(centers)), dtype=int)\n",
    "\n",
    "    if peaks.size == 0:\n",
    "        return []\n",
    "\n",
    "    # 3) cluster contiguous peak indices into single tick centers (gap clustering)\n",
    "    gap_px = max(1, int(w * tick_min_gap_frac))\n",
    "    clusters = []\n",
    "    cur = [peaks[0]]\n",
    "    for p in peaks[1:]:\n",
    "        if p - cur[-1] <= gap_px:\n",
    "            cur.append(p)\n",
    "        else:\n",
    "            clusters.append(int(np.median(cur)))\n",
    "            cur = [p]\n",
    "    clusters.append(int(np.median(cur)))\n",
    "    tick_xs = [int(c) for c in clusters]\n",
    "\n",
    "    # 4) OCR labels below axis and associate by proximity\n",
    "    label_h = int(h * ocr_label_h_frac)\n",
    "    ly0 = axis_y + 2\n",
    "    ly1 = min(h, axis_y + 2 + label_h)\n",
    "    if ly0 >= h or ly0 >= ly1:\n",
    "        # no label area\n",
    "        labels = []\n",
    "    else:\n",
    "        crop = img_bgr[ly0:ly1, :]\n",
    "        pil = Image.fromarray(crop[:,:,::-1])\n",
    "        ocr = pytesseract.image_to_data(pil, output_type=pytesseract.Output.DICT)\n",
    "        labels = []\n",
    "        for i, txt in enumerate(ocr['text']):\n",
    "            t = txt.strip()\n",
    "            if not t:\n",
    "                continue\n",
    "            left = ocr['left'][i]\n",
    "            top = ocr['top'][i]\n",
    "            width = ocr['width'][i]\n",
    "            cx = left + width/2\n",
    "            cy = top + ocr['height'][i]/2 + ly0\n",
    "            labels.append({\"text\": t, \"cx\": cx, \"cy\": cy, \"bbox\": (left, top+ly0, left+width, top+ly0+ocr['height'][i])})\n",
    "\n",
    "    # Cluster/merge OCR boxes horizontally to make label candidates (DBSCAN if available)\n",
    "    label_clusters = []\n",
    "    if labels:\n",
    "        xs = np.array([l['cx'] for l in labels]).reshape(-1,1)\n",
    "        if _have_dbscan:\n",
    "            db = DBSCAN(eps=max(6, w*0.02), min_samples=1).fit(xs)\n",
    "            for cid in np.unique(db.labels_):\n",
    "                members = [labels[i] for i,lab in enumerate(db.labels_) if lab==cid]\n",
    "                tx = \" \".join([m['text'] for m in members])\n",
    "                cx = np.mean([m['cx'] for m in members])\n",
    "                bb0 = min([m['bbox'][0] for m in members])\n",
    "                bb1 = min([m['bbox'][1] for m in members])\n",
    "                bb2 = max([m['bbox'][2] for m in members])\n",
    "                bb3 = max([m['bbox'][3] for m in members])\n",
    "                label_clusters.append({\"text\": tx, \"cx\": cx, \"bbox\": (bb0, bb1, bb2, bb3)})\n",
    "        else:\n",
    "            # simple gap-based cluster\n",
    "            labels_sorted = sorted(labels, key=lambda x: x['cx'])\n",
    "            cur = [labels_sorted[0]]\n",
    "            for lb in labels_sorted[1:]:\n",
    "                if lb['cx'] - cur[-1]['cx'] <= max(6, w*0.02):\n",
    "                    cur.append(lb)\n",
    "                else:\n",
    "                    tx = \" \".join([m['text'] for m in cur])\n",
    "                    cx = np.mean([m['cx'] for m in cur])\n",
    "                    label_clusters.append({\"text\": tx, \"cx\": cx, \"bbox\": (min([m['bbox'][0] for m in cur]), min([m['bbox'][1] for m in cur]), max([m['bbox'][2] for m in cur]), max([m['bbox'][3] for m in cur]))})\n",
    "                    cur = [lb]\n",
    "            if cur:\n",
    "                tx = \" \".join([m['text'] for m in cur])\n",
    "                cx = np.mean([m['cx'] for m in cur])\n",
    "                label_clusters.append({\"text\": tx, \"cx\": cx, \"bbox\": (min([m['bbox'][0] for m in cur]), min([m['bbox'][1] for m in cur]), max([m['bbox'][2] for m in cur]), max([m['bbox'][3] for m in cur]))})\n",
    "\n",
    "    # 5) Associate tick xs to nearest label cluster (within half inter-tick gap)\n",
    "    if len(tick_xs) > 1:\n",
    "        inter_gap = np.median(np.diff(sorted(tick_xs)))\n",
    "    else:\n",
    "        inter_gap = w * 0.1\n",
    "    max_assoc_dist = max(8, inter_gap * 0.6)\n",
    "\n",
    "    results = []\n",
    "    for tx in tick_xs:\n",
    "        assoc = None\n",
    "        if label_clusters:\n",
    "            dists = [abs(tx - lc['cx']) for lc in label_clusters]\n",
    "            best_i = int(np.argmin(dists))\n",
    "            if dists[best_i] <= max_assoc_dist:\n",
    "                assoc = label_clusters[best_i]['text']\n",
    "                bbox = label_clusters[best_i]['bbox']\n",
    "            else:\n",
    "                assoc = None\n",
    "                bbox = (tx-3, axis_y-3, tx+3, axis_y+3)\n",
    "        else:\n",
    "            assoc = None\n",
    "            bbox = (tx-3, axis_y-3, tx+3, axis_y+3)\n",
    "        results.append({\"x\": int(tx), \"label\": assoc, \"bbox\": tuple(map(int, bbox))})\n",
    "\n",
    "    # sort by x\n",
    "    results = sorted(results, key=lambda r: r['x'])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c5c2f",
   "metadata": {},
   "source": [
    "When the ticks are detected, these must be labeled.\n",
    "\n",
    "1. With the starting and ending months known, the month numbers in between are deducible.\n",
    "2. These month numbers must be aligned with the horizontal position of the tick. \n",
    "3. Any existing labels must be replaced by the labels generated from the tick positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58dbfea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_ticks(img_bgr, ticks, color=(0,0,255), erase_pad=6, sample_h=12):\n",
    "    \"\"\"\n",
    "    Draw replacement labels while erasing any original X-axis labels/marks.\n",
    "    - Erase original label areas (using 'bbox' or 'tick_bbox') but do NOT erase the detected axis line.\n",
    "    - Sample a small patch above the label bbox to estimate background color.\n",
    "    - Draw replacement labels so that the first character of each label\n",
    "      starts exactly at the tick x coordinate (left-aligned).\n",
    "    - Put a common baseline close to the horizontal axis so all labels are horizontally aligned.\n",
    "    \"\"\"\n",
    "    out = img_bgr.copy()\n",
    "    h, w = out.shape[:2]\n",
    "\n",
    "    # infer axis_y from provided ticks (prefer explicit y_axis)\n",
    "    axis_candidates = [int(t.get(\"y_axis\")) for t in ticks if t.get(\"y_axis\") is not None]\n",
    "    axis_y = int(np.median(axis_candidates)) if axis_candidates else None\n",
    "\n",
    "    # If axis_y still unknown, try to infer it from label/tick bboxes:\n",
    "    if axis_y is None:\n",
    "        bboxes = [t.get(\"bbox\") or t.get(\"tick_bbox\") for t in ticks if (t.get(\"bbox\") or t.get(\"tick_bbox\"))]\n",
    "        if bboxes:\n",
    "            # labels' top coord (bbox[1]) is usually just below axis -> axis slightly above that\n",
    "            tops = [int(b[1]) for b in bboxes]\n",
    "            axis_y = max(0, min(tops) - 4)  # small offset above the top of label bboxes\n",
    "        else:\n",
    "            axis_y = None\n",
    "\n",
    "    # First pass: erase original label areas (per-tick) but never erase the axis line.\n",
    "    for t in ticks:\n",
    "        bbox = t.get(\"bbox\") or t.get(\"tick_bbox\")\n",
    "        if not bbox:\n",
    "            continue\n",
    "        x0, y0, x1, y1 = map(int, bbox)\n",
    "        # expand bbox a little to remove nearby artifacts / vertical grid fragments\n",
    "        x0e = max(0, x0 - erase_pad)\n",
    "        x1e = min(w, x1 + erase_pad)\n",
    "        y0e = max(0, y0 - erase_pad)\n",
    "        y1e = min(h, y1 + erase_pad)\n",
    "\n",
    "        # ensure we do not erase the horizontal axis line (if known)\n",
    "        if axis_y is not None:\n",
    "            # if erase rect would cross or touch the axis, clamp it to start strictly below the axis\n",
    "            if y0e <= axis_y:\n",
    "                y0e = axis_y + 1\n",
    "            # if after clamping there's nothing to erase, skip\n",
    "            if y0e >= y1e:\n",
    "                continue\n",
    "\n",
    "        # try sampling a small strip just above the label bbox to estimate background color\n",
    "        # but avoid sampling across the axis\n",
    "        sy1 = y0e\n",
    "        sy0 = max(0, sy1 - sample_h)\n",
    "        if axis_y is not None and sy0 <= axis_y < sy1:\n",
    "            # move sampling region to be above axis (or just below if not possible)\n",
    "            sy0 = max(0, axis_y - sample_h)\n",
    "            sy1 = axis_y\n",
    "            if sy0 >= sy1:\n",
    "                sy0 = max(0, y1e - sample_h)\n",
    "                sy1 = y1e\n",
    "\n",
    "        sample = out[sy0:sy1, x0e:x1e] if sy1 > sy0 and x1e > x0e else None\n",
    "        if sample is not None and sample.size:\n",
    "            med = np.median(sample.reshape(-1, 3), axis=0).astype(int)\n",
    "            bg_color = (int(med[0]), int(med[1]), int(med[2]))\n",
    "        else:\n",
    "            bg_color = (255, 255, 255)\n",
    "\n",
    "        cv2.rectangle(out, (x0e, y0e), (x1e, y1e), bg_color, thickness=-1)\n",
    "\n",
    "    # Optional: also erase a continuous band spanning all label bboxes to remove residuals,\n",
    "    # but make sure band does not include the axis line.\n",
    "    try:\n",
    "        bboxes = [t.get(\"bbox\") or t.get(\"tick_bbox\") for t in ticks if (t.get(\"bbox\") or t.get(\"tick_bbox\"))]\n",
    "        if bboxes:\n",
    "            lefts = [int(b[0]) for b in bboxes]\n",
    "            rights = [int(b[2]) for b in bboxes]\n",
    "            tops = [int(b[1]) for b in bboxes]\n",
    "            bots = [int(b[3]) for b in bboxes]\n",
    "            band_x0 = max(0, min(lefts) - erase_pad)\n",
    "            band_x1 = min(w, max(rights) + erase_pad)\n",
    "            band_y0 = max(0, min(tops) - erase_pad)\n",
    "            band_y1 = min(h, max(bots) + erase_pad)\n",
    "\n",
    "            # ensure band does not include axis_y\n",
    "            if axis_y is not None and band_y0 <= axis_y <= band_y1:\n",
    "                band_y0 = axis_y + 1\n",
    "                if band_y0 >= band_y1:\n",
    "                    raise ValueError(\"band would erase axis; skip band erase\")\n",
    "\n",
    "            sy0b = max(0, band_y0 - sample_h)\n",
    "            sample_band = out[sy0b:band_y0, band_x0:band_x1] if band_y0 > sy0b and band_x1 > band_x0 else None\n",
    "            if sample_band is not None and sample_band.size:\n",
    "                med = np.median(sample_band.reshape(-1, 3), axis=0).astype(int)\n",
    "                bg_color = (int(med[0]), int(med[1]), int(med[2]))\n",
    "            else:\n",
    "                bg_color = (255,255,255)\n",
    "            cv2.rectangle(out, (band_x0, band_y0), (band_x1, band_y1), bg_color, thickness=-1)\n",
    "    except Exception:\n",
    "        # if any geometry fails or would erase axis, ignore band erase\n",
    "        pass\n",
    "\n",
    "    # Determine a common baseline (y) for all labels — place it just below the axis if axis available,\n",
    "    # otherwise use the bottoms of original bbox regions.\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.5\n",
    "    thickness = 1\n",
    "\n",
    "    if axis_y is not None:\n",
    "        # baseline nominally a short distance below axis\n",
    "        baseline_y = min(h - 6, axis_y + 16)\n",
    "    else:\n",
    "        bottoms = []\n",
    "        for t in ticks:\n",
    "            bbox = t.get(\"bbox\") or t.get(\"tick_bbox\")\n",
    "            if bbox:\n",
    "                bottoms.append(int(bbox[3]))\n",
    "            elif t.get(\"y_axis\") is not None:\n",
    "                bottoms.append(int(t.get(\"y_axis\")) + 10)\n",
    "        if bottoms:\n",
    "            baseline_y = min(h - 6, max(bottoms) + 14)\n",
    "        else:\n",
    "            baseline_y = h - 20\n",
    "    baseline_y = max(12, int(baseline_y))\n",
    "\n",
    "    # Second pass: draw replacement labels (left-aligned so first char sits at tick x),\n",
    "    # using the common baseline_y and ensuring text never overlaps the axis.\n",
    "    for t in ticks:\n",
    "        lbl = (t.get(\"label\") or \"\").strip() or str(t.get(\"x\", \"\"))\n",
    "        x = int(t.get(\"x\", 0))\n",
    "\n",
    "        # compute text size\n",
    "        (text_w, text_h), baseline = cv2.getTextSize(lbl, font, font_scale, thickness)\n",
    "\n",
    "        # left-align text: first character x coordinate == tick x\n",
    "        x_text = x\n",
    "        # clamp so text stays inside image (allow text to start at x even if it extends to right)\n",
    "        x_text = max(2, min(x_text, w - text_w - 2))\n",
    "\n",
    "        # determine y_text baseline: keep common baseline but ensure it's below axis + text height\n",
    "        y_text = int(baseline_y)\n",
    "        if axis_y is not None:\n",
    "            min_ok = axis_y + text_h + 6\n",
    "            if y_text - text_h <= axis_y:\n",
    "                y_text = max(y_text, min_ok)\n",
    "            else:\n",
    "                # still ensure minimum clearance\n",
    "                y_text = max(y_text, min_ok)\n",
    "        else:\n",
    "            y_text = y_text\n",
    "\n",
    "        y_text = min(h - 4, y_text)\n",
    "\n",
    "        # background rectangle for readability\n",
    "        pad = 2\n",
    "        rect_x0 = x_text - pad\n",
    "        rect_y0 = y_text - text_h - pad\n",
    "        rect_x1 = x_text + text_w + pad\n",
    "        rect_y1 = y_text + baseline + pad\n",
    "        rect_x0 = max(0, rect_x0)\n",
    "        rect_y0 = max(0, rect_y0)\n",
    "        rect_x1 = min(w, rect_x1)\n",
    "        rect_y1 = min(h, rect_y1)\n",
    "\n",
    "        # sample bg color just above text rect to blend, avoid sampling axis\n",
    "        sy0 = max(0, rect_y0 - sample_h)\n",
    "        sy1 = rect_y0\n",
    "        if axis_y is not None and sy0 <= axis_y < sy1:\n",
    "            sy0 = max(0, axis_y - sample_h)\n",
    "            sy1 = axis_y\n",
    "            if sy0 >= sy1:\n",
    "                sy0 = max(0, rect_y1 - sample_h)\n",
    "                sy1 = rect_y1\n",
    "\n",
    "        sample = out[sy0:sy1, rect_x0:rect_x1] if sy1 > sy0 and rect_x1 > rect_x0 else None\n",
    "        if sample is not None and sample.size:\n",
    "            med = np.median(sample.reshape(-1, 3), axis=0).astype(int)\n",
    "            bg_color = (int(med[0]), int(med[1]), int(med[2]))\n",
    "        else:\n",
    "            bg_color = (255,255,255)\n",
    "\n",
    "        cv2.rectangle(out, (rect_x0, rect_y0), (rect_x1, rect_y1), bg_color, thickness=-1)\n",
    "        cv2.putText(out, lbl, (int(x_text), int(y_text)), font, font_scale, (0,0,0), thickness, cv2.LINE_AA)\n",
    "\n",
    "    return out\n",
    "\n",
    "def detect_month_index_from_text(s):\n",
    "    if not s:\n",
    "        return None\n",
    "    ss = s.lower()\n",
    "    mnum = re.search(r'\\b(1[0-2]|[1-9])\\b', ss)\n",
    "    if mnum:\n",
    "        try:\n",
    "            val = int(mnum.group(0))\n",
    "            if 1 <= val <= 12:\n",
    "                return val - 1\n",
    "        except Exception:\n",
    "            pass\n",
    "    for token, idx in month_name_to_idx.items():\n",
    "        if token in ss:\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "def fallback_find_months_in_band(img_bgr, axis_y=None, search_frac=(0.65,0.99)):\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    if axis_y:\n",
    "        ly0 = min(h-1, axis_y - 6)\n",
    "    else:\n",
    "        ly0 = int(h * search_frac[0])\n",
    "    ly1 = min(h, int(h * search_frac[1]))\n",
    "    band = img_bgr[ly0:ly1, :]\n",
    "    pil = Image.fromarray(band[:,:,::-1])\n",
    "    ocr = pytesseract.image_to_data(pil, output_type=pytesseract.Output.DICT)\n",
    "    found = []\n",
    "    for i, txt in enumerate(ocr['text']):\n",
    "        t = txt.strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        left = ocr['left'][i]\n",
    "        width = ocr['width'][i]\n",
    "        cx = left + width/2\n",
    "        idx = detect_month_index_from_text(t)\n",
    "        if idx is not None:\n",
    "            found.append({\"x\": int(cx), \"label\": t, \"month_idx\": idx, \"abs_x\": int(cx)})\n",
    "        else:\n",
    "            # also accept text that contains month substring (defensive)\n",
    "            tl = t.lower()\n",
    "            for token, mi in month_name_to_idx.items():\n",
    "                if token in tl:\n",
    "                    found.append({\"x\": int(cx), \"label\": t, \"month_idx\": mi, \"abs_x\": int(cx)})\n",
    "                    break\n",
    "    return found, (ly0, ly1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee09f78a",
   "metadata": {},
   "source": [
    "### Convert charts to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eaba5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "from chart_to_structured import preprocess_for_deplot, run_deplot\n",
    "\n",
    "def process_markdown(md_path, out_path=None, overwrite=False, save_debug=False):\n",
    "    \"\"\"\n",
    "    Process markdown keeping only two image-related items:\n",
    "      - Replace the single image that appears immediately before the phrase\n",
    "        \"Your annual electricity use\" with deplot-extracted table (preprocess_for_deplot -> run_deplot).\n",
    "        The deplot extracted text is post-processed: 'Year' -> 'kWh' and numeric first-column values\n",
    "        (1-12) are replaced by month names.\n",
    "      - Ensure the image link that appears immediately after the phrase\n",
    "        \"than similar nearby homes\" is present right after that phrase (inserted if necessary).\n",
    "        That particular image is NOT preprocessed/deplot'ed — it is inserted as-is.\n",
    "    All other lines containing image links are deleted.\n",
    "    \"\"\"\n",
    "    md_path = _resolve_md_path_input(md_path)\n",
    "    md_path = Path(md_path)\n",
    "    if not md_path.exists():\n",
    "        ok = _ensure_markdown_generated(md_path)\n",
    "        if not ok:\n",
    "            print(\"Markdown not found and demo generation failed; aborting.\")\n",
    "            return\n",
    "\n",
    "    text = md_path.read_text(encoding=\"utf-8\")\n",
    "    img_matches = list(re.finditer(r'!\\[.*?\\]\\(([^)]+)\\)', text))\n",
    "    if not img_matches:\n",
    "        print(\"No image links found.\")\n",
    "        # still write cleaned output (no images to remove)\n",
    "        out_file = md_path if overwrite else (Path(out_path) if out_path else md_path.with_name(md_path.stem + \"_processed.md\"))\n",
    "        out_file.write_text(text, encoding=\"utf-8\")\n",
    "        print(\"Wrote:\", out_file)\n",
    "        return\n",
    "\n",
    "    # Identify phrase locations in original text\n",
    "    phrase1 = \"Your annual electricity use\"\n",
    "    phrase2 = \"than similar nearby homes\"\n",
    "    orig_phrase1_pos = text.lower().find(phrase1.lower())\n",
    "    orig_phrase2_pos = text.lower().find(phrase2.lower())\n",
    "\n",
    "    # determine the target image before phrase1 (if any)\n",
    "    target_before_m = None\n",
    "    if orig_phrase1_pos != -1:\n",
    "        candidates = [m for m in img_matches if m.end() <= orig_phrase1_pos]\n",
    "        if candidates:\n",
    "            target_before_m = max(candidates, key=lambda m: m.start())\n",
    "\n",
    "    # determine the image immediately after phrase2 (if any)\n",
    "    after_phrase2_m = None\n",
    "    if orig_phrase2_pos != -1:\n",
    "        post_candidates = [m for m in img_matches if m.start() >= orig_phrase2_pos]\n",
    "        if post_candidates:\n",
    "            after_phrase2_m = min(post_candidates, key=lambda m: m.start())\n",
    "\n",
    "    # Build cleaned text by removing all lines that contain image links,\n",
    "    # except we do NOT keep the original lines for the two special images\n",
    "    # (they will be handled/inserted explicitly).\n",
    "    lines = text.splitlines()\n",
    "    cleaned_lines = []\n",
    "    # prepare canonical markdown snippets for comparison\n",
    "    target_before_md = text[target_before_m.start():target_before_m.end()] if target_before_m else None\n",
    "    after_phrase2_md = text[after_phrase2_m.start():after_phrase2_m.end()] if after_phrase2_m else None\n",
    "\n",
    "    img_re = re.compile(r'!\\[.*?\\]\\(([^)]+)\\)')\n",
    "    for ln in lines:\n",
    "        m = img_re.search(ln)\n",
    "        if not m:\n",
    "            cleaned_lines.append(ln)\n",
    "            continue\n",
    "        # line contains an image link\n",
    "        # drop any image lines; the after-phrase2 image will be inserted later if needed,\n",
    "        # and the target-before image will be replaced by the extracted block.\n",
    "        continue\n",
    "\n",
    "    new_text = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    device = get_device()\n",
    "    print(\"Device:\", device)\n",
    "    deplot_proc, deplot_model = load_models(device)\n",
    "\n",
    "\n",
    "    # --- process target_before image: run preprocess_for_deplot -> deplot, then insert extracted block\n",
    "    if target_before_m:\n",
    "        img_rel = target_before_m.group(1).strip()\n",
    "        # only attempt processing for local images\n",
    "        if img_rel.startswith(\"http://\") or img_rel.startswith(\"https://\"):\n",
    "            print(f\"Target-before image is remote ({img_rel}) — will not run deplot; no extracted block inserted.\")\n",
    "        else:\n",
    "            img_path = (md_path.parent / img_rel).resolve()\n",
    "            if not img_path.exists():\n",
    "                print(f\"Missing target-before image: {img_path} — skipping extraction.\")\n",
    "            else:\n",
    "                try:\n",
    "                    pil = Image.open(img_path).convert(\"RGB\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Cannot open {img_path}: {e}. Skipping extraction.\")\n",
    "                    pil = None\n",
    "\n",
    "                if pil is not None:\n",
    "                    device = get_device()\n",
    "                    deplot_proc = deplot_model = None\n",
    "                    models_loaded = False\n",
    "                    try:\n",
    "                        from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration\n",
    "                        deplot_proc = Pix2StructProcessor.from_pretrained(\"google/deplot\")\n",
    "                        deplot_model = Pix2StructForConditionalGeneration.from_pretrained(\"google/deplot\").to(device)\n",
    "                        models_loaded = True\n",
    "                    except Exception as e:\n",
    "                        print(\"Failed to load deplot model:\", e)\n",
    "                        models_loaded = False\n",
    "\n",
    "                    extracted = None\n",
    "                    if models_loaded:\n",
    "                        try:\n",
    "                            prepped_pil, did_pre = preprocess_for_deplot(pil, img_path=img_path, save_debug=save_debug)\n",
    "                            extracted = run_deplot(prepped_pil, deplot_proc, deplot_model, device)\n",
    "                            if (not extracted or len(extracted.strip()) < 8) and did_pre:\n",
    "                                print(\"deplot returned little/empty on annotated image — retrying with original image.\")\n",
    "                                extracted = run_deplot(pil, deplot_proc, deplot_model, device)\n",
    "                        except Exception as e:\n",
    "                            print(f\"deplot failed for {img_rel}: {e}. Leaving no extracted block.\")\n",
    "                            extracted = None\n",
    "\n",
    "                        # post-process extracted text:\n",
    "                        def transform_extracted_text(txt):\n",
    "                            # replace Year -> kWh (word-boundary, case-insensitive)\n",
    "                            txt = re.sub(r'\\bYear\\b', 'kWh', txt, flags=re.IGNORECASE)\n",
    "                            MONTHS = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "\n",
    "                            out_lines = []\n",
    "                            for ln in txt.splitlines():\n",
    "                                if not ln.strip():\n",
    "                                    out_lines.append(ln)\n",
    "                                    continue\n",
    "\n",
    "                                # try markdown table row with pipes\n",
    "                                if '|' in ln:\n",
    "                                    parts = ln.split('|')\n",
    "                                    # find first non-empty cell index\n",
    "                                    first_idx = None\n",
    "                                    for i, cell in enumerate(parts):\n",
    "                                        if cell.strip() != '':\n",
    "                                            first_idx = i\n",
    "                                            break\n",
    "                                    if first_idx is not None:\n",
    "                                        first = parts[first_idx].strip()\n",
    "                                        mnum = re.match(r'^(\\d{1,2})\\b', first)\n",
    "                                        if mnum:\n",
    "                                            n = int(mnum.group(1))\n",
    "                                            if 1 <= n <= 12:\n",
    "                                                parts[first_idx] = ' ' + MONTHS[n-1] + ' '\n",
    "                                                out_lines.append('|'.join(parts))\n",
    "                                                continue\n",
    "                                    # if no replacement done, fall through to append original\n",
    "                                    out_lines.append(ln)\n",
    "                                    continue\n",
    "\n",
    "                                # CSV-like (comma separated)\n",
    "                                if ',' in ln:\n",
    "                                    cells = [c for c in ln.split(',')]\n",
    "                                    first = cells[0].strip()\n",
    "                                    mnum = re.match(r'^(\\d{1,2})\\b', first)\n",
    "                                    if mnum:\n",
    "                                        n = int(mnum.group(1))\n",
    "                                        if 1 <= n <= 12:\n",
    "                                            cells[0] = MONTHS[n-1]\n",
    "                                            out_lines.append(','.join(cells))\n",
    "                                            continue\n",
    "                                    out_lines.append(ln)\n",
    "                                    continue\n",
    "\n",
    "                                # whitespace separated / plain row: replace leading integer token if present\n",
    "                                toks = ln.lstrip()\n",
    "                                leading_ws = ln[:len(ln)-len(toks)]\n",
    "                                toks_split = toks.split()\n",
    "                                if toks_split:\n",
    "                                    mnum = re.match(r'^(\\d{1,2})\\b', toks_split[0])\n",
    "                                    if mnum:\n",
    "                                        n = int(mnum.group(1))\n",
    "                                        if 1 <= n <= 12:\n",
    "                                            # replace only the first token while preserving original leading whitespace and remainder\n",
    "                                            replaced = re.sub(r'^(\\s*)\\d{1,2}\\b', r'\\1' + MONTHS[n-1], ln, count=1)\n",
    "                                            out_lines.append(replaced)\n",
    "                                            continue\n",
    "                                out_lines.append(ln)\n",
    "                            return '\\n'.join(out_lines)\n",
    "\n",
    "                        processed_extracted = transform_extracted_text(extracted)\n",
    "                        replacement = f\"\\n\\n```\\\\n{processed_extracted}\\\\n```\\\\n\\\\n\"\n",
    "                    else:\n",
    "                        replacement = f\"\\n\\n```\\\\n_deplot produced no textual output_\\\\n```\\\\n\\\\n\"\n",
    "\n",
    "                    # Insert into cleaned text at the location that follows the line after the original image line in the ORIGINAL markdown.\n",
    "                    # Strategy: locate the \"line after\" content in the original 'text', then find that same line in new_text and insert after it.\n",
    "                    try:\n",
    "                        orig = text  # original markdown (before cleaning)\n",
    "                        # compute the end of the original image line\n",
    "                        img_line_end = orig.find('\\n', target_before_m.end())\n",
    "                        if img_line_end == -1:\n",
    "                            # no following line => append at end of cleaned text\n",
    "                            new_text = new_text + replacement\n",
    "                            print(\"No following line after original image; appended extracted block.\")\n",
    "                        else:\n",
    "                            # start of next line\n",
    "                            next_line_start = img_line_end + 1\n",
    "                            next_line_end = orig.find('\\n', next_line_start)\n",
    "                            if next_line_end == -1:\n",
    "                                next_line = orig[next_line_start:].rstrip('\\n')\n",
    "                            else:\n",
    "                                next_line = orig[next_line_start:next_line_end]\n",
    "\n",
    "                            if next_line:\n",
    "                                # find that line in the cleaned/filtered new_text\n",
    "                                pos = new_text.find(next_line)\n",
    "                                if pos != -1:\n",
    "                                    insert_pos = pos + len(next_line)\n",
    "                                    new_text = new_text[:insert_pos] + replacement + new_text[insert_pos:]\n",
    "                                    print(\"Inserted extracted block after the original image's following line.\")\n",
    "                                else:\n",
    "                                    # fallback: insert before phrase1 if present, else prepend\n",
    "                                    if phrase1.lower() in new_text.lower():\n",
    "                                        insert_pos = new_text.lower().find(phrase1.lower())\n",
    "                                        new_text = new_text[:insert_pos] + replacement + new_text[insert_pos:]\n",
    "                                        print(\"Could not find original following line in cleaned text; inserted before phrase1 as fallback.\")\n",
    "                                    else:\n",
    "                                        new_text = replacement + new_text\n",
    "                                        print(\"Could not find original following line in cleaned text; prepended extracted block as fallback.\")\n",
    "                            else:\n",
    "                                # next_line empty -> append\n",
    "                                new_text = new_text + replacement\n",
    "                                print(\"Following line empty; appended extracted block.\")\n",
    "                    except Exception as e:\n",
    "                        print(\"Error inserting extracted block at desired location:\", e)\n",
    "                        # fallback to insert before phrase or prepend\n",
    "                        if phrase1.lower() in new_text.lower():\n",
    "                            insert_pos = new_text.lower().find(phrase1.lower())\n",
    "                            new_text = new_text[:insert_pos] + replacement + new_text[insert_pos:]\n",
    "                        else:\n",
    "                            new_text = replacement + new_text\n",
    "                        print(\"Fallback insertion performed.\")\n",
    "    else:\n",
    "        print(f'No image found immediately before phrase \"{phrase1}\" — no extraction performed.')\n",
    "\n",
    "    # --- ensure the image link immediately after phrase2 is processed with deplot (insert extracted block)\n",
    "    if after_phrase2_m and after_phrase2_md:\n",
    "        img_rel2 = after_phrase2_m.group(1).strip()\n",
    "        # only attempt processing for local images\n",
    "        if img_rel2.startswith(\"http://\") or img_rel2.startswith(\"https://\"):\n",
    "            print(f\"Image after phrase2 is remote ({img_rel2}) — skipping deplot; no insertion.\")\n",
    "        else:\n",
    "            img_path2 = (md_path.parent / img_rel2).resolve()\n",
    "            if not img_path2.exists():\n",
    "                print(f\"Missing image after phrase2: {img_path2} — skipping insertion.\")\n",
    "            else:\n",
    "                try:\n",
    "                    pil2 = Image.open(img_path2).convert(\"RGB\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Cannot open {img_path2}: {e}. Skipping insertion.\")\n",
    "                    pil2 = None\n",
    "\n",
    "                if pil2 is not None:\n",
    "                    device = get_device()\n",
    "                    deplot_proc2 = deplot_model2 = None\n",
    "                    models_loaded2 = False\n",
    "                    try:\n",
    "                        from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration\n",
    "                        deplot_proc2 = Pix2StructProcessor.from_pretrained(\"google/deplot\")\n",
    "                        deplot_model2 = Pix2StructForConditionalGeneration.from_pretrained(\"google/deplot\").to(device)\n",
    "                        models_loaded2 = True\n",
    "                    except Exception as e:\n",
    "                        print(\"Failed to load deplot model for phrase2 image:\", e)\n",
    "                        models_loaded2 = False\n",
    "\n",
    "                    extracted2 = None\n",
    "                    if models_loaded2:\n",
    "                        try:\n",
    "                            prepped_pil2, did_pre2 = preprocess_for_deplot(pil2, img_path=img_path2, save_debug=save_debug)\n",
    "                            extracted2 = run_deplot(prepped_pil2, deplot_proc2, deplot_model2, device)\n",
    "                            if (not extracted2 or len(extracted2.strip()) < 8) and did_pre2:\n",
    "                                print(\"deplot returned little/empty on annotated image — retrying with original image.\")\n",
    "                                extracted2 = run_deplot(pil2, deplot_proc2, deplot_model2, device)\n",
    "                            #extracted2 = run_deplot(pil2, deplot_proc2, deplot_model2, device)\n",
    "                        except Exception as e:\n",
    "                            print(f\"deplot failed for {img_rel2}: {e}. Leaving no extracted block.\")\n",
    "                            extracted2 = None\n",
    "\n",
    "                    if extracted2:\n",
    "                        # For the phrase2 insertion we also perform Year->kWh replacement for consistency\n",
    "                        #extracted2 = re.sub(r'\\bYear\\b', 'kWh', extracted2, flags=re.IGNORECASE)\n",
    "                        replacement2 = f\"\\n\\n```\\\\n{extracted2}\\\\n```\\\\n\\\\n\"\n",
    "                    else:\n",
    "                        replacement2 = f\"\\n\\n```\\\\n_deplot produced no textual output_\\\\n```\\\\n\\\\n\"\n",
    "\n",
    "                    # insert extracted block immediately after phrase2 (or append if phrase2 not found)\n",
    "                    if phrase2.lower() in new_text.lower():\n",
    "                        phrase2_pos = new_text.lower().find(phrase2.lower())\n",
    "                        insert_pos = phrase2_pos + len(phrase2)\n",
    "                        new_text = new_text[:insert_pos] + replacement2 + new_text[insert_pos:]\n",
    "                        print(f\"Inserted extracted block for image after phrase '{phrase2}': {img_rel2}\")\n",
    "                    else:\n",
    "                        new_text = new_text + replacement2\n",
    "                        print(f\"Phrase '{phrase2}' not found; appended its extracted block at document end.\")\n",
    "    else:\n",
    "        print(f\"No image found immediately after phrase '{phrase2}' — nothing to insert for that phrase.\")\n",
    "\n",
    "    # final: write cleaned-and-processed output\n",
    "    if overwrite:\n",
    "        out_file = md_path\n",
    "    else:\n",
    "        out_file = Path(out_path) if out_path else md_path.with_name(md_path.stem + \"_processed.md\")\n",
    "    out_file.write_text(new_text, encoding=\"utf-8\")\n",
    "    print(\"Wrote:\", out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e12934",
   "metadata": {},
   "source": [
    "## Generate the context from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0938c71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-07 01:47:17.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmineru.backend.pipeline.pipeline_analyze\u001b[0m:\u001b[36mdoc_analyze\u001b[0m:\u001b[36m128\u001b[0m - \u001b[1mBatch 1/1: 2 pages/2 pages\u001b[0m\n",
      "\u001b[32m2025-11-07 01:47:17.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmineru.backend.pipeline.model_init\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mDocAnalysis init, this may take some times......\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45d35d5ca914c85b2002fcd1dd31ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c0ec61a81b4849b0519600e8b4557f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69540a55d099456cb7503fee6f44bf9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b5284decf441d28d2735bc8902ea96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f65a8d971c641fb88409e9e37a5f73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9568bfd4a3454621b48b9eb7db4de398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fdda42fe404623aa309874178cc3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acb67b8508b41e9937830647778124a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89005822eeed4b9f9004253a8e3dcb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed991827499a405198c31a0d2f1ec0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371330fc327744489f4b37e26fa79e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726c40a7bb234ab1a347430fe5ea8fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34d6aab4ab345a89299066b7dd98885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-07 01:47:27.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmineru.backend.pipeline.model_init\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m270\u001b[0m - \u001b[1mDocAnalysis init done!\u001b[0m\n",
      "\u001b[32m2025-11-07 01:47:27.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmineru.backend.pipeline.pipeline_analyze\u001b[0m:\u001b[36mcustom_model_init\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mmodel init cost: 9.689947128295898\u001b[0m\n",
      "Layout Predict: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\n",
      "MFD Predict: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n",
      "MFR Predict: 100%|██████████| 2/2 [00:00<00:00,  5.37it/s]\n",
      "Table-ocr det: 0it [00:00, ?it/s]\n",
      "Table-wireless Predict: 0it [00:00, ?it/s]\n",
      "OCR-det Predict:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8aa23dbf7f746939dc6c54b47cb7ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fef75a5e7cc42f3b7b0b5de9a41b51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OCR-det Predict: 100%|██████████| 2/2 [00:10<00:00,  5.09s/it]\n",
      "\u001b[32m2025-11-07 01:47:41.462\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmineru_demo\u001b[0m:\u001b[36mdo_parse\u001b[0m:\u001b[36m139\u001b[0m - \u001b[34m\u001b[1mCould not open image-list entry, keeping entry: <class 'dict'>\u001b[0m\n",
      "\u001b[32m2025-11-07 01:47:41.463\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmineru_demo\u001b[0m:\u001b[36mdo_parse\u001b[0m:\u001b[36m139\u001b[0m - \u001b[34m\u001b[1mCould not open image-list entry, keeping entry: <class 'dict'>\u001b[0m\n",
      "Processing pages:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec16b8ab5ef411c8b4ad8ca9c245197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n",
      "\u001b[32m2025-11-07 01:47:43.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmineru_demo\u001b[0m:\u001b[36m_process_output\u001b[0m:\u001b[36m245\u001b[0m - \u001b[1mlocal output dir is /Users/earl/Documents/test_rag_ph/MinerU/demo/output/test_info_extract/auto\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran demo.parse_doc; waiting for markdown generation.\n",
      "Device: mps\n",
      "Inserted extracted block after the original image's following line.\n",
      "Inserted extracted block for image after phrase 'than similar nearby homes': images/cc50f95fa5997e6351965fb16e335b4d1038587ef538831d6853fea680a4f593.jpg\n",
      "Wrote: /Users/earl/Documents/test_rag_ph/MinerU/demo/output/test_info_extract/auto/test_info_extract_processed.md\n"
     ]
    }
   ],
   "source": [
    "process_markdown(\"test_info_extract\",\n",
    "                 out_path=None,\n",
    "                 overwrite=False,\n",
    "                 save_debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5c0f5",
   "metadata": {},
   "source": [
    "### Interact with the PDF\n",
    "\n",
    "1. The markdown from the PDF feeds context to the LLM `qwen3:4b` on `ollama`.\n",
    "2. Query the LLM by typing in your query in the line assigning `ask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc4c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the **monthly electricity usage data** provided in your Home Energy Report (specifically the \"Your annual electricity use compared with similar and efficient homes\" table), **December** shows the peak electricity usage for your home.  \n",
      "\n",
      "Here's the clear breakdown of your monthly usage (in kWh) for the user (\"You\"):\n",
      "\n",
      "| Month       | Your Usage (kWh) |\n",
      "|-------------|------------------|\n",
      "| April       | 115              |\n",
      "| May         | 141              |\n",
      "| June        | 160              |\n",
      "| July        | 174              |\n",
      "| August      | 168              |\n",
      "| September   | 150              |\n",
      "| October     | 160              |\n",
      "| November    | 170              |\n",
      "| **December**| **181**          |\n",
      "| January     | 160              |\n",
      "| February    | 126              |\n",
      "| March       | 114              |\n",
      "\n",
      "### Why December is the peak:\n",
      "- **December has the highest value at 181 kWh** (the only month exceeding 174 kWh).\n",
      "- This is higher than all other months, including July (174 kWh) and November (170 kWh).\n",
      "- The report shows this is consistent with your typical usage pattern (you use 18% more than similar nearby homes).\n",
      "\n",
      "### Important context:\n",
      "- The peak here refers to **monthly total electricity usage**, not hourly demand (which would be a different metric). The report focuses on monthly consumption as shown in the table.\n",
      "- December's high usage aligns with typical seasonal patterns (e.g., heating in colder months, holiday-related activity), but the data clearly shows it as your highest month.\n",
      "\n",
      "**Answer: December** is the month with peak electricity usage.  \n",
      "\n",
      "*Note: If you're looking for hourly peak times (e.g., \"when is demand highest in a day?\"), the report doesn’t provide that detail—it only shows monthly totals. For real-time peak analysis, you’d need a different tool like your utility’s peak demand report.*"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "md_path_processed = _resolve_md_path_input(\"test_info_extract\")\n",
    "md_path_processed = Path(md_path_processed).with_name(Path(md_path_processed).stem + \"_processed.md\")\n",
    "\n",
    "with open(md_path_processed, \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "############# TYPE YOUR QUESTION BELOW #############\n",
    "ask = 'In which month does the electricity usage peak?'\n",
    "###################################################\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/generate\",\n",
    "    json={\n",
    "        \"model\": \"qwen3:4b\",\n",
    "        \"prompt\": content+ask,\n",
    "        \"stream\": True\n",
    "    },\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        data = json.loads(line.decode(\"utf-8\"))\n",
    "        if \"response\" in data:\n",
    "            print(data[\"response\"], end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377a240",
   "metadata": {},
   "source": [
    "## Limitations and Improvements\n",
    "\n",
    "1. This solution was tested locally in Apple Silicon M3 for now. \n",
    "2. The chart conversion to table has substantial room for improvement, especially for diverse chart types (e.g., bar chart, line, scatterplot, etc.)\n",
    "3. We utilize a relatively small (**4B parameters**) LLM for efficiency. Quantized models may be faster, but larger ones may provide better responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2debf5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (test_rag)",
   "language": "python",
   "name": "test_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
